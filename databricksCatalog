from pyspark.sql import SparkSession, Row

# create a SparkSession
spark = SparkSession.builder.appName("ShowTablesInfo").getOrCreate()

# set the database
spark.catalog.setCurrentDatabase("default")

# get all tables
tables = spark.catalog.listTables()

# create a list of dictionaries containing the table information
table_info = []
for table in tables:
    if table.tableType == 'MANAGED' or table.tableType == 'EXTERNAL':
        database = table.database
        name = table.name
        location = spark.sql(f"DESCRIBE EXTENDED {name}").filter("Location").select("data_type").collect()[0][0]
        table_info.append({"Database": database, "Table": name, "Location": location})

# create a DataFrame from the list of dictionaries
df = spark.createDataFrame([Row(**x) for x in table_info])

# show the DataFrame
df.show()

# stop the SparkSession
spark.stop()
